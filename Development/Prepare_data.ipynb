{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_into_df(folder_name: Path, original_prisma_data: bool, move_to_archive: bool) -> list:\n",
    "    '''\n",
    "    This function searches for all .xls files in a given directory, loads each file into a Pandas dataframe and changes the header line.\n",
    "    If move_to_archive is set True, then all processed files will be moved to the archive.\n",
    "    return: List with all created dataframes\n",
    "    '''\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        logger.error(f\"The path {folder_name} does not exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.info(\"Loading the data...\")\n",
    "\n",
    "        # Create an empty list to store all dataframes\n",
    "        dataframes = []\n",
    "        \n",
    "        # Loop through all files in the folder and open them as dataframes\n",
    "        for file in os.listdir(folder_name):\n",
    "            if file.endswith(\".xls\") or file.endswith(\".xlsx\"):\n",
    "                try:\n",
    "                    # Load the excel into a pandas dataframe, delete the header and declare the second row as new header\n",
    "                    if original_prisma_data == True:\n",
    "                        df = pd.read_excel(os.path.join(folder_name, file), header=None, skiprows=1)\n",
    "                        #df = pd.read_excel(os.path.join(folder_name, file), skiprows=0)\n",
    "                        df.columns = df.iloc[0]\n",
    "                        df = df.iloc[1:]\n",
    "                    else:\n",
    "                        df = pd.read_excel(os.path.join(folder_name, file))\n",
    "\n",
    "                    # Add the created dataframe to the list of dataframes\n",
    "                    dataframes.append(df)\n",
    "\n",
    "                    if move_to_archive == True:\n",
    "                        # Move file to archive\n",
    "                        shutil.move(os.path.join(folder_name, file), os.path.join(folder_name, \"original_data_archive\", file))\n",
    "\n",
    "                except:\n",
    "                    logger.info(f\"Error reading file {file}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "    # Check if any dataframes were created\n",
    "    if len(dataframes) == 0:\n",
    "        logger.error(f\"No dataframes were created - please check if the files in folder {folder_name} are correct/exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.success(f\"{len(dataframes)} dataframe(s) were created.\")\n",
    "\n",
    "        return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function takes a list of data frames as input and checks if the dataframes have the same header. If so, the dataframes will be merged.\n",
    "    return: Merged dataframe\n",
    "    '''\n",
    "    # Set the header information\n",
    "    columns_set = set(dataframes[0].columns)\n",
    "\n",
    "    # Check if all dataframes have the same columns \n",
    "    for df in dataframes:\n",
    "        if set(df.columns) != columns_set:\n",
    "            print(df.columns)\n",
    "            print(columns_set)\n",
    "            raise ValueError(\"All dataframes must have the same columns.\")\n",
    "    \n",
    "    # Merge all dataframes into a single dataframe\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    merged_df.to_excel(\"../data/combined_dataset.xlsx\")\n",
    "\n",
    "    logger.success(f\"{len(dataframes)} dataframe(s) are combined to one dataset and stored in a excel file.\")\n",
    "    \n",
    "    return merged_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info_to_excel(df: pd.DataFrame):\n",
    "    '''\n",
    "    This function saves feature informations in an excel file\n",
    "    '''\n",
    "    pd.DataFrame({\"name\": df.columns, \"non-nulls\": len(df)-df.isnull().sum().values, \"nulls\": df.isnull().sum().values, \"type\": df.dtypes.values}).to_excel(\"data_infos.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_add_labels(data_folder_dir: Path, original_prisma_data: bool, save_as_excel: bool, move_to_archive: bool):\n",
    "\n",
    "    # Load the data into a list of pandas dataframes\n",
    "    dataframes = load_csv_into_df(data_folder_dir, original_prisma_data, move_to_archive)\n",
    "\n",
    "    # Store the ncar abbreviation for file paths\n",
    "    ncar = dataframes[0]['Benennung (dt)'][1][:3]\n",
    "\n",
    "    logger.info(\"Start preprocessing the data...\")\n",
    "    dataframes_with_labels = []\n",
    "    for i in range(len(dataframes)):\n",
    "        # Keep only the relevant samples with Dok-Format=5P. This samples are on the last level of the car structure\n",
    "        dataframes[i] = dataframes[i][dataframes[i][\"Dok-Format\"]=='5P'].reset_index(drop=True)\n",
    "\n",
    "        # Keep only features which are identified as relevant for the preprocessing, the predictions or for the users' next steps\n",
    "        dataframes[i] = dataframes[i][['Sachnummer','Benennung (dt)', 'X-Min','X-Max','Y-Min','Y-Max','Z-Min','Z-Max', 'Wert','Einheit','Gewichtsart','Kurzname','L-Kz.', 'L/R-Kz.', 'Modul (Nr)', 'ox','oy', 'oz', 'xx','xy','xz', 'yx','yy','yz','zx','zy','zz']]\n",
    "\n",
    "        # using dictionary to convert specific columns\n",
    "        convert_dict = {'X-Min': float,\n",
    "                        'X-Max': float,\n",
    "                        'Y-Min': float,\n",
    "                        'Y-Max': float,\n",
    "                        'Z-Min': float,\n",
    "                        'Z-Max': float,\n",
    "                        'Wert': float,\n",
    "                        'ox': float,\n",
    "                        'oy': float,\n",
    "                        'oz': float,\n",
    "                        'xx': float,\n",
    "                        'xy': float,\n",
    "                        'xz': float,\n",
    "                        'yx': float,\n",
    "                        'yy': float,\n",
    "                        'yz': float,\n",
    "                        'zx': float,\n",
    "                        'zy': float,\n",
    "                        'zz': float                     \n",
    "                        }\n",
    "        \n",
    "        dataframes[i] = dataframes[i].astype(convert_dict)\n",
    "\n",
    "        # Add columns for the label \"Relevant fÃ¼r Messung\" and \"Allgemeine Bezeichnung\"\n",
    "        data_labeled = dataframes[i]\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Relevant fuer Messung', 'Nein')\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Einheitsname', 'Dummy')\n",
    "        dataframes_with_labels.append(data_labeled)\n",
    "\n",
    "        if save_as_excel==True:\n",
    "            # Date\n",
    "            dateTimeObj = datetime.now()\n",
    "            timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "            \n",
    "            # Store preprocessed dataframes\n",
    "            dataframes_with_labels[i].to_excel(f\"../data/preprocessed_data/{ncar}_preprocessed_{timestamp}.xlsx\")\n",
    "\n",
    "    if save_as_excel == True:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type. The new dataset is stored as {ncar}_preprocessed_{timestamp}.xlsx!\")\n",
    "    else:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type!\")\n",
    "    \n",
    "    return dataframes_with_labels, ncar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(designation: str) -> str:\n",
    "    # transform to lower case\n",
    "    text = str(designation).upper()\n",
    "\n",
    "    # Removing punctations\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # tokenize text\n",
    "    text = text.split(\" \")\n",
    "\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "\n",
    "    # join all\n",
    "    prepared_designation = \" \".join(text)\n",
    "\n",
    "    return prepared_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data: pd.DataFrame, timestamp) -> tuple:\n",
    "    token = WhitespaceTokenizer()\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=token.tokenize)\n",
    "\n",
    "    X_text = vectorizer.fit_transform(data['Benennung (dt)']).toarray()\n",
    "\n",
    "    # Store the vocabulary\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Save the vectorizer and vocabulary to files\n",
    "    with open(f'../models/vectorizer_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(f'../models/vocabulary_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "\n",
    "    return X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val(df, test_size:float, timestamp):\n",
    "    df[\"Benennung (dt)\"] = df.apply(lambda x: prepare_text(x[\"Benennung (dt)\"]), axis=1)\n",
    "\n",
    "    #vectorizer = CountVectorizer()\n",
    "    #X_text = vectorizer.fit_transform(df['Benennung (dt)']).toarray()\n",
    "\n",
    "    X_text = vectorize_data(df, timestamp)\n",
    "\n",
    "    # Combine text features with other features\n",
    "    X = np.concatenate((X_text, df[['center_x', 'center_y', 'center_z','length','width','height','theta_x','theta_y','theta_z']].values), axis=1)\n",
    "\n",
    "    y = df['Relevant fuer Messung']\n",
    "    y = y.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the path to the folder containing the data (xls files)\n",
    "    data_folder = Path(\"../data/original_data_new\")\n",
    "\n",
    "    df, ncar = prepare_and_add_labels(data_folder, original_prisma_data=True, save_as_excel=True, move_to_archive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-10 16:26:11.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
      "\u001b[32m2023-05-10 16:26:13.008\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m46\u001b[0m - \u001b[32m\u001b[1m1 dataframe(s) were created.\u001b[0m\n",
      "\u001b[32m2023-05-10 16:26:13.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_add_labels\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mStart preprocessing the data...\u001b[0m\n",
      "\u001b[32m2023-05-10 16:26:17.183\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_add_labels\u001b[0m:\u001b[36m57\u001b[0m - \u001b[32m\u001b[1mThe features are reduced and formated to the correct data type. The new dataset is stored as G23_preprocessed_10052023_1626.xlsx!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMesstool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
