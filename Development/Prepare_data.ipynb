{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_into_df(folder_name: Path, move_to_archive: bool) -> list:\n",
    "    '''\n",
    "    This function searches for all .xls files in a given directory, loads each file into a Pandas dataframe and changes the header line.\n",
    "    If move_to_archive is set True, then all processed files will be moved to the archive.\n",
    "    return: List with all created dataframes\n",
    "    '''\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        logger.error(f\"The path {folder_name} does not exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.info(\"Loading the data...\")\n",
    "\n",
    "        # Create an empty list to store all dataframes\n",
    "        dataframes = []\n",
    "        \n",
    "        # Loop through all files in the folder and open them as dataframes\n",
    "        for file in os.listdir(folder_name):\n",
    "            if file.endswith(\".xls\") or file.endswith(\".xlsx\"):\n",
    "                try:\n",
    "                    # Load the excel into a pandas dataframe, delete the header and declare the second row as new header\n",
    "                    df = pd.read_excel(os.path.join(folder_name, file), header=None, skiprows=1)\n",
    "           \n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df.iloc[1:]\n",
    "                    \n",
    "                    # Add the created dataframe to the list of dataframes\n",
    "                    dataframes.append(df)\n",
    "\n",
    "                    if move_to_archive == True:\n",
    "                        # Move file to archive\n",
    "                        shutil.move(os.path.join(folder_name, file), os.path.join(folder_name, \"original_data_archive\", file))\n",
    "\n",
    "                except:\n",
    "                    logger.info(f\"Error reading file {file}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "    # Check if any dataframes were created\n",
    "    if len(dataframes) == 0:\n",
    "        logger.error(f\"No dataframes were created - please check if the files in folder {folder_name} are correct/exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.success(f\"{len(dataframes)} dataframe(s) were created.\")\n",
    "\n",
    "        return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function takes a list of data frames as input and checks if the dataframes have the same header. If so, the dataframes will be merged.\n",
    "    return: Merged dataframe\n",
    "    '''\n",
    "    # Set the header information\n",
    "    columns_set = set(dataframes[0].columns)\n",
    "\n",
    "    # Check if all dataframes have the same columns \n",
    "    for df in dataframes:\n",
    "        if set(df.columns) != columns_set:\n",
    "            raise ValueError(\"All dataframes must have the same columns.\")\n",
    "    \n",
    "    # Merge all dataframes into a single dataframe\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    merged_df.to_excel(\"../data/combined_dataset.xlsx\")\n",
    "\n",
    "    logger.success(f\"{len(dataframes)} dataframe(s) are combined to one dataset and stored in a excel file.\")\n",
    "    \n",
    "    return merged_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info_to_excel(df: pd.DataFrame):\n",
    "    '''\n",
    "    This function saves feature informations in an excel file\n",
    "    '''\n",
    "    pd.DataFrame({\"name\": df.columns, \"non-nulls\": len(df)-df.isnull().sum().values, \"nulls\": df.isnull().sum().values, \"type\": df.dtypes.values}).to_excel(\"data_infos.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_and_add_labels\u001b[39m(data_folder_dir: Path, save_as_excel: \u001b[39mbool\u001b[39m, move_to_archive: \u001b[39mbool\u001b[39m):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[39m# Load the data into a list of pandas dataframes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     dataframes \u001b[39m=\u001b[39m load_csv_into_df(data_folder_dir, move_to_archive)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Store the ncar abbreviation for file paths\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_and_add_labels(data_folder_dir: Path, save_as_excel: bool, move_to_archive: bool):\n",
    "\n",
    "    # Load the data into a list of pandas dataframes\n",
    "    dataframes = load_csv_into_df(data_folder_dir, move_to_archive)\n",
    "\n",
    "    # Store the ncar abbreviation for file paths\n",
    "    ncar = dataframes[0]['Benennung (dt)'][1][:3]\n",
    "\n",
    "    logger.info(\"Start preprocessing the data...\")\n",
    "  \n",
    "    dataframes_with_labels = []\n",
    "    for i in range(len(dataframes)):\n",
    "        # Keep only the relevant samples with Dok-Format=5P. This samples are on the last level of the car structure\n",
    "        dataframes[i] = dataframes[i][dataframes[i][\"Dok-Format\"]=='5P'].reset_index(drop=True)\n",
    "\n",
    "        # Keep only features which are identified as relevant for the preprocessing, the predictions or for the users' next steps\n",
    "        dataframes[i] = dataframes[i][['Sachnummer','Benennung (dt)', 'X-Min','X-Max','Y-Min','Y-Max','Z-Min','Z-Max', 'Wert','Einheit','Gewichtsart','Kurzname','L-Kz.', 'L/R-Kz.', 'Modul (Nr)', 'ox','oy', 'oz', 'xx','xy','xz', 'yx','yy','yz','zx','zy','zz']]\n",
    "\n",
    "        # using dictionary to convert specific columns\n",
    "        convert_dict = {'X-Min': float,\n",
    "                        'X-Max': float,\n",
    "                        'Y-Min': float,\n",
    "                        'Y-Max': float,\n",
    "                        'Z-Min': float,\n",
    "                        'Z-Max': float,\n",
    "                        'Wert': float,\n",
    "                        'ox': float,\n",
    "                        'oy': float,\n",
    "                        'oz': float,\n",
    "                        'xx': float,\n",
    "                        'xy': float,\n",
    "                        'xz': float,\n",
    "                        'yx': float,\n",
    "                        'yy': float,\n",
    "                        'yz': float,\n",
    "                        'zx': float,\n",
    "                        'zy': float,\n",
    "                        'zz': float                     \n",
    "                        }\n",
    "        \n",
    "        dataframes[i] = dataframes[i].astype(convert_dict)\n",
    "\n",
    "        # Add columns for the label \"Relevant fÃ¼r Messung\" and \"Allgemeine Bezeichnung\"\n",
    "        data_labeled = dataframes[i]\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Relevant fuer Messung', 'Nein')\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Einheitsname', 'Dummy')\n",
    "        dataframes_with_labels.append(data_labeled)\n",
    "\n",
    "        if save_as_excel==True:\n",
    "            # Date\n",
    "            dateTimeObj = datetime.now()\n",
    "            timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "            \n",
    "            # Store preprocessed dataframes\n",
    "            dataframes_with_labels[i].to_excel(f\"../data/preprocessed_data/{ncar}_preprocessed_{timestamp}.xlsx\")\n",
    "\n",
    "    if save_as_excel == True:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type. The new dataset is stored as {ncar}_preprocessed_{timestamp}.xlsx!\")\n",
    "    else:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type!\")\n",
    "    \n",
    "    return dataframes_with_labels, ncar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(designation: str) -> str:\n",
    "    # transform to lower case\n",
    "    text = str(designation).upper()\n",
    "\n",
    "    # Removing punctations\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # tokenize text\n",
    "    text = text.split(\" \")\n",
    "\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "\n",
    "    # join all\n",
    "    prepared_designation = \" \".join(text)\n",
    "\n",
    "    return prepared_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data: pd.DataFrame, timestamp) -> tuple:\n",
    "    token = WhitespaceTokenizer()\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=token.tokenize)\n",
    "\n",
    "    X_text = vectorizer.fit_transform(data['Benennung (dt)']).toarray()\n",
    "\n",
    "    # Store the vocabulary\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Save the vectorizer and vocabulary to files\n",
    "    with open(f'../models/vectorizer_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(f'../models/vocabulary_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "\n",
    "    return X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val(df, test_size:float, timestamp):\n",
    "    df[\"Benennung (dt)\"] = df.apply(lambda x: prepare_text(x[\"Benennung (dt)\"]), axis=1)\n",
    "\n",
    "    #vectorizer = CountVectorizer()\n",
    "    #X_text = vectorizer.fit_transform(df['Benennung (dt)']).toarray()\n",
    "\n",
    "    X_text = vectorize_data(df, timestamp)\n",
    "\n",
    "    # Combine text features with other features\n",
    "    X = np.concatenate((X_text, df[['center_x', 'center_y', 'center_z','length','width','height','theta_x','theta_y','theta_z']].values), axis=1)\n",
    "\n",
    "    y = df['Relevant fuer Messung']\n",
    "    y = y.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test,y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the path to the folder containing the data (xls files)\n",
    "    data_folder = Path(\"../data/test\")\n",
    "\n",
    "    prepare_and_add_labels(data_folder, save_as_excel=True, move_to_archive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-03 12:17:55.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
      "\u001b[32m2023-05-03 12:17:57.496\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1m1 dataframe(s) were created.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0  5A58C83       LI SCHRAUBLEISTE VERBINDUNG AUSSEN    393.34552   \n",
      "1        1  5A58C84       RE SCHRAUBLEISTE VERBINDUNG AUSSEN   393.345520  \\\n",
      "2        2  7057717        VERSTAERKUNG FKL SCHARNIER HINTEN   -24.951582   \n",
      "3        3  7057717        VERSTAERKUNG FKL SCHARNIER HINTEN   -24.951582   \n",
      "4        4  7057717        VERSTAERKUNG FKL SCHARNIER HINTEN   -24.951582   \n",
      "5        5  7057717        VERSTAERKUNG FKL SCHARNIER HINTEN   -24.951582   \n",
      "...    ...      ...                                      ...          ...   \n",
      "4580  4580  7240308  KOMBI-LINSENSCHRAUBE MIT ISA UND ZAPFEN    -5.992137   \n",
      "4581  4581  5A6D584                            DMU BIM-01 CN   -51.970592   \n",
      "4582  4582  8865032                           ZB GELENKWELLE -2000.000000   \n",
      "4583  4583  4A0FF84    LU RE ATW ICE SX AAR2600I 72MM VAG170  -143.796768   \n",
      "4584  4584  4A25628     LU RE AW IRP8.1-41 HAG225AL V210 NAF  2890.083496   \n",
      "\n",
      "0       422.042664       -827.0  -802.402954  -124.375999    67.050003   \n",
      "1       422.042664   802.402954   827.000000  -124.375999    67.050003  \\\n",
      "2        24.951582    -8.900000    29.883780     0.000000    10.750000   \n",
      "3        24.951582    -8.900000    29.883780     0.000000    10.750000   \n",
      "4        24.951582    -8.900000    29.883780     0.000000    10.750000   \n",
      "5        24.951582    -8.900000    29.883780     0.000000    10.750000   \n",
      "...            ...          ...          ...          ...          ...   \n",
      "4580      5.992137    -5.910232     5.910232   -16.500000     4.700000   \n",
      "4581     61.535782   -75.568840    75.568840   -63.200001     3.075871   \n",
      "4582  10000.000000 -2400.000000  2000.000000 -2000.000000  3000.000000   \n",
      "4583      3.125736  -832.631226   832.631287  -194.883209    96.559448   \n",
      "4584   3017.361572  -812.065735   812.065674  -198.154602    93.606308   \n",
      "\n",
      "0      104.0  ... -2.43213555635459e-07       0.0 2.43213555649311e-07   \n",
      "1      104.0  ...          3.443079e-07  0.000000        -3.443079e-07  \\\n",
      "2       21.0  ...         -4.544976e-16  0.416277        -4.162770e-01   \n",
      "3       21.0  ...         -2.822303e-16 -0.691627         6.916267e-01   \n",
      "4       21.0  ...          2.873180e-11  0.412506        -4.125055e-01   \n",
      "5       21.0  ...          3.060474e-16 -0.675767         6.757675e-01   \n",
      "...      ...  ...                   ...       ...                  ...   \n",
      "4580     5.0  ...         -6.236776e-03  0.259175         6.236776e-03   \n",
      "4581   400.0  ...          0.000000e+00  0.241922         0.000000e+00   \n",
      "4582  9368.0  ...          0.000000e+00  0.000000         0.000000e+00   \n",
      "4583  7000.0  ...          0.000000e+00  0.000000         0.000000e+00   \n",
      "4584  7078.0  ...          0.000000e+00  0.000000         0.000000e+00   \n",
      "\n",
      "0              1.0       0.0           0.0       0.0           1.0  Nein  NaN  \n",
      "1     1.000000e+00  0.000000  0.000000e+00  0.000000  1.000000e+00  Nein  NaN  \n",
      "2    -2.161757e-10 -0.909238 -8.998956e-11  1.000000 -1.965560e-10  Nein  NaN  \n",
      "3     1.099013e-15 -0.722255  1.936718e-15 -1.000000 -1.182696e-15  Nein  NaN  \n",
      "4     2.142578e-10 -0.910955 -1.145561e-10 -1.000000 -1.833283e-10  Nein  NaN  \n",
      "5    -1.092620e-15 -0.737115  1.923794e-15  1.000000 -1.203698e-15  Nein  NaN  \n",
      "...            ...       ...           ...       ...           ...   ...  ...  \n",
      "4580  9.988623e-01  0.047278 -2.591753e-01  0.047278 -9.646725e-01  Nein  NaN  \n",
      "4581  1.000000e+00  0.000000 -2.419219e-01  0.000000  9.702957e-01  Nein  NaN  \n",
      "4582  1.000000e+00  0.000000  0.000000e+00  0.000000  1.000000e+00  Nein  NaN  \n",
      "4583  1.000000e+00  0.000000  0.000000e+00  0.000000  1.000000e+00  Nein  NaN  \n",
      "4584  1.000000e+00  0.000000  0.000000e+00  0.000000  1.000000e+00  Nein  NaN  \n",
      "\n",
      "[4584 rows x 30 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Benennung (dt)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\_libs\\index.pyx:171\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\_libs\\index.pyx:214\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\_libs\\index.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\_libs\\index.pyx:114\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Benennung (dt)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Define the path to the folder containing the data (xls files)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     data_folder \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39m../data/test\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     prepare_and_add_labels(data_folder, save_as_excel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, move_to_archive\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mprepare_and_add_labels\u001b[1;34m(data_folder_dir, save_as_excel, move_to_archive)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(dataframes[\u001b[39m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[39m# Store the ncar abbreviation for file paths\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m ncar \u001b[39m=\u001b[39m dataframes[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mBenennung (dt)\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m1\u001b[39m][:\u001b[39m3\u001b[39m]\n\u001b[0;32m     11\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mStart preprocessing the data...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m dataframes_with_labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\q617269\\AppData\\Local\\anaconda3\\envs\\envMesstool\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Benennung (dt)'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMesstool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
