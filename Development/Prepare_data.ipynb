{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_into_df(folder_name: Path, original_prisma_data: bool, move_to_archive: bool) -> list:\n",
    "    '''\n",
    "    This function searches for all .xls files in a given directory, loads each file into a Pandas dataframe and changes the header line.\n",
    "    If move_to_archive is set True, then all processed files will be moved to the archive.\n",
    "    return: List with all created dataframes\n",
    "    '''\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        logger.error(f\"The path {folder_name} does not exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.info(\"Loading the data...\")\n",
    "\n",
    "        # Create an empty list to store all dataframes\n",
    "        dataframes = []\n",
    "        \n",
    "        # Loop through all files in the folder and open them as dataframes\n",
    "        for file in os.listdir(folder_name):\n",
    "            if file.endswith(\".xls\") or file.endswith(\".xlsx\"):\n",
    "                try:\n",
    "                    # Load the excel into a pandas dataframe, delete the header and declare the second row as new header\n",
    "                    if original_prisma_data == True:\n",
    "                        df = pd.read_excel(os.path.join(folder_name, file), header=None, skiprows=1)\n",
    "                        df.columns = df.iloc[0]\n",
    "                        df = df.iloc[1:]\n",
    "                    else:\n",
    "                        df = pd.read_excel(os.path.join(folder_name, file))\n",
    "\n",
    "                    # Add the created dataframe to the list of dataframes\n",
    "                    dataframes.append(df)\n",
    "\n",
    "                    if move_to_archive == True:\n",
    "                        # Move file to archive\n",
    "                        shutil.move(os.path.join(folder_name, file), os.path.join(folder_name, \"original_data_archive\", file))\n",
    "\n",
    "                except:\n",
    "                    logger.info(f\"Error reading file {file}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "    # Check if any dataframes were created\n",
    "    if len(dataframes) == 0:\n",
    "        logger.error(f\"No dataframes were created - please check if the files in folder {folder_name} are correct/exist.\")\n",
    "        exit()\n",
    "    else:\n",
    "        logger.success(f\"{len(dataframes)} dataframe(s) were created.\")\n",
    "\n",
    "        return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function takes a list of data frames as input and checks if the dataframes have the same header. If so, the dataframes will be merged.\n",
    "    return: Merged dataframe\n",
    "    '''\n",
    "    # Set the header information\n",
    "    columns_set = set(dataframes[0].columns)\n",
    "\n",
    "    # Check if all dataframes have the same columns \n",
    "    for df in dataframes:\n",
    "        if set(df.columns) != columns_set:\n",
    "            print(df.columns)\n",
    "            print(columns_set)\n",
    "            raise ValueError(\"All dataframes must have the same columns.\")\n",
    "    \n",
    "    # Merge all dataframes into a single dataframe\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    logger.success(f\"{len(dataframes)} dataframe(s) are combined to one dataset.\")\n",
    "    \n",
    "    return merged_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info_to_excel(df: pd.DataFrame):\n",
    "    '''\n",
    "    This function saves feature informations in an excel file\n",
    "    '''\n",
    "    pd.DataFrame({\"name\": df.columns, \"non-nulls\": len(df)-df.isnull().sum().values, \"nulls\": df.isnull().sum().values, \"type\": df.dtypes.values}).to_excel(\"data_infos.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_add_labels(dataframes: list, save_as_excel: bool):\n",
    "\n",
    "    logger.info(\"Start preprocessing the data...\")\n",
    "    dataframes_with_labels = []\n",
    "    ncars = []\n",
    "\n",
    "    for i in range(len(dataframes)):\n",
    "        # Store the ncar abbreviation for file paths\n",
    "        ncar = dataframes[i]['Benennung (dt)'][1][:3]\n",
    "        ncars.append(ncar)\n",
    "\n",
    "        # Temporary store the modul for the interior mirror\n",
    "        level_interor_mirror = dataframes[i][dataframes[i]['Benennung (dt)'].str.startswith(f'{ncar} CE05')][\"Ebene\"].values[0]\n",
    "        startindex_interor_mirror = dataframes[i][dataframes[i]['Benennung (dt)'].str.startswith(f'{ncar} CE05')].index[-1]+1\n",
    "        endindex_interor_mirror = dataframes[i].loc[(dataframes[i][\"Ebene\"] == level_interor_mirror) & (dataframes[i].index > startindex_interor_mirror)].index[0]-1\n",
    "        temp_interor_mirror = dataframes[i].loc[startindex_interor_mirror:endindex_interor_mirror]\n",
    "\n",
    "        # Temporary store the modul for the interior mirror\n",
    "        level_roof_antenna = dataframes[i][dataframes[i]['Benennung (dt)'].str.startswith(f'{ncar} CD07')][\"Ebene\"].values[0]\n",
    "        startindex_roof_antenna = dataframes[i][dataframes[i]['Benennung (dt)'].str.startswith(f'{ncar} CD07')].index[-1]+1\n",
    "        endindex_roof_antenna = dataframes[i].loc[(dataframes[i][\"Ebene\"] == level_roof_antenna) & (dataframes[i].index > startindex_roof_antenna)].index[0]-1\n",
    "        temp_roof_antenna = dataframes[i].loc[startindex_roof_antenna:endindex_roof_antenna]\n",
    "\n",
    "        # Keep only car parts of module group EP\n",
    "        index_EF_module = dataframes[i][dataframes[i]['Benennung (dt)'].str.startswith(f'EF {ncar}')].index[-1]\n",
    "        dataframes[i] = dataframes[i].loc[:index_EF_module-1]\n",
    "\n",
    "        # Add interor mirror \n",
    "        dataframes[i] = pd.concat([dataframes[i], temp_interor_mirror]).reset_index(drop=True)\n",
    "    \n",
    "        # Add roof antenna \n",
    "        dataframes[i] = pd.concat([dataframes[i], temp_roof_antenna]).reset_index(drop=True)\n",
    "\n",
    "        # Keep only the relevant samples with Dok-Format=5P. This samples are on the last level of the car structure\n",
    "        dataframes[i] = dataframes[i][dataframes[i][\"Dok-Format\"]=='5P'].reset_index(drop=True)\n",
    "\n",
    "        # Delete the NCAR abbreviation because of data security reasons\n",
    "        dataframes[i][\"Benennung (dt)\"] = dataframes[i][\"Benennung (dt)\"].apply(lambda x: x.replace(ncar, \"\"))\n",
    "\n",
    "        # Keep only features which are identified as relevant for the preprocessing, the predictions or for the users' next steps\n",
    "        dataframes[i] = dataframes[i][['Sachnummer','Benennung (dt)', 'X-Min','X-Max','Y-Min','Y-Max','Z-Min','Z-Max', 'Wert','Einheit','Gewichtsart','Kurzname','L-Kz.', 'L/R-Kz.', 'Modul (Nr)', 'ox','oy', 'oz', 'xx','xy','xz', 'yx','yy','yz','zx','zy','zz']]\n",
    "\n",
    "        # using dictionary to convert specific columns\n",
    "        convert_dict = {'X-Min': float,\n",
    "                        'X-Max': float,\n",
    "                        'Y-Min': float,\n",
    "                        'Y-Max': float,\n",
    "                        'Z-Min': float,\n",
    "                        'Z-Max': float,\n",
    "                        'Wert': float,\n",
    "                        'ox': float,\n",
    "                        'oy': float,\n",
    "                        'oz': float,\n",
    "                        'xx': float,\n",
    "                        'xy': float,\n",
    "                        'xz': float,\n",
    "                        'yx': float,\n",
    "                        'yy': float,\n",
    "                        'yz': float,\n",
    "                        'zx': float,\n",
    "                        'zy': float,\n",
    "                        'zz': float                     \n",
    "                        }\n",
    "        \n",
    "        dataframes[i] = dataframes[i].astype(convert_dict)\n",
    "\n",
    "        # Add columns for the label \"Relevant fÃ¼r Messung\" and \"Allgemeine Bezeichnung\"\n",
    "        data_labeled = dataframes[i]\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Relevant fuer Messung', 'Nein')\n",
    "        data_labeled.insert(len(data_labeled.columns), 'Einheitsname', 'Dummy')\n",
    "        dataframes_with_labels.append(data_labeled)\n",
    "\n",
    "        if save_as_excel==True:\n",
    "            # Date\n",
    "            dateTimeObj = datetime.now()\n",
    "            timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "            \n",
    "            # Store preprocessed dataframes\n",
    "            dataframes_with_labels[i].to_excel(f\"../data/preprocessed_data/{ncar}_preprocessed_{timestamp}.xlsx\")\n",
    "\n",
    "    if save_as_excel == True:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type. The new dataset is stored as {ncar}_preprocessed_{timestamp}.xlsx!\")\n",
    "    else:\n",
    "        logger.success(f\"The features are reduced and formated to the correct data type!\")\n",
    "    \n",
    "    return dataframes_with_labels, ncars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(designation: str) -> str:\n",
    "    # transform to lower case\n",
    "    text = str(designation).upper()\n",
    "\n",
    "    # Removing punctations\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Removing numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # tokenize text\n",
    "    text = text.split(\" \")\n",
    "\n",
    "    # Remove predefined words\n",
    "    predefined_words = ['ZB', 'AF', 'LI', 'RE', 'MD', 'LL', 'TAB', 'TB']\n",
    "    if len(predefined_words) > 0:\n",
    "        text = [word for word in text if word not in predefined_words]\n",
    "\n",
    "    # Remove words with only one letter\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "\n",
    "    # join all\n",
    "    prepared_designation = \" \".join(text)\n",
    "\n",
    "    return prepared_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data: pd.DataFrame, df_val, timestamp) -> tuple:\n",
    "    #token = WhitespaceTokenizer()\n",
    "    #vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=token.tokenize)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 8))\n",
    "\n",
    "    X_text = vectorizer.fit_transform(data['Benennung (dt)']).toarray()\n",
    "    X_test = vectorizer.transform(df_val['Benennung (dt)']).toarray()\n",
    "\n",
    "    # Store the vocabulary\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Save the vectorizer and vocabulary to files\n",
    "    os.makedirs(f'../models/lgbm_{timestamp}')\n",
    "    with open(f'../models/lgbm_{timestamp}/vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(f'../models/lgbm_{timestamp}/vocabulary.pkl', 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "\n",
    "    return X_text, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df[\"Benennung (dt)\"] = df.apply(lambda x: prepare_text(x[\"Benennung (dt)\"]), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val(df, df_test,only_text: bool, test_size:float, timestamp):\n",
    "    \n",
    "    X, X_test = vectorize_data(df, df_test, timestamp)\n",
    "\n",
    "    # Combine text features with other features\n",
    "    features = ['center_x', 'center_y', 'center_z','length','width','height','theta_x','theta_y','theta_z']\n",
    "    if only_text == False:\n",
    "        X = np.concatenate((X, df[features].values), axis=1)\n",
    "        X_test = np.concatenate((X_test, df_test[features].values), axis=1)\n",
    "\n",
    "    y = df['Relevant fuer Messung']\n",
    "    y = y.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    y_test = df_test['Relevant fuer Messung']\n",
    "    y_test = y_test.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_kfold(df, df_test, timestamp):\n",
    "    df[\"Benennung (dt)\"] = df.apply(lambda x: prepare_text(x[\"Benennung (dt)\"]), axis=1)\n",
    "    df_test[\"Benennung (dt)\"] = df_test.apply(lambda x: prepare_text(x[\"Benennung (dt)\"]), axis=1)\n",
    "\n",
    "    X, X_test = vectorize_data(df, df_test, timestamp)\n",
    "\n",
    "    # Combine text features with other features\n",
    "    features = ['center_x', 'center_y', 'center_z','length','width','height','theta_x','theta_y','theta_z']\n",
    "    #X = np.concatenate((X, df[features].values), axis=1)\n",
    "\n",
    "    y = df['Relevant fuer Messung']\n",
    "    y = y.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    y_test = df_test['Relevant fuer Messung']\n",
    "    y_test = y_test.map({'Ja': 1, 'Nein': 0})\n",
    "\n",
    "    return X, y, X_test, y_test, features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the path to the folder containing the data (xls files)\n",
    "    data_folder = Path(\"../data/labeled_data\")\n",
    "    dataframes = load_csv_into_df(data_folder, original_prisma_data=False, move_to_archive=False)\n",
    "    df = combine_dataframes(dataframes)\n",
    "    print(df.shape)\n",
    "    #df, ncar = prepare_and_add_labels(dataframes, save_as_excel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 22:18:58.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
      "\u001b[32m2023-05-24 22:19:04.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mError reading file ~$G14_labeled.xlsx. Skipping...\u001b[0m\n",
      "\u001b[32m2023-05-24 22:19:04.413\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m45\u001b[0m - \u001b[32m\u001b[1m8 dataframe(s) were created.\u001b[0m\n",
      "\u001b[32m2023-05-24 22:19:04.421\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcombine_dataframes\u001b[0m:\u001b[36m19\u001b[0m - \u001b[32m\u001b[1m8 dataframe(s) are combined to one dataset.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23765, 30)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMesstool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
