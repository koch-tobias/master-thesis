{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from Prepare_data import train_test_val, train_test_val_kfold, load_csv_into_df, combine_dataframes\n",
    "from Feature_Engineering import preprocess_dataset\n",
    "from Data_Augmentation import data_augmentation\n",
    "from config import lgbm_params\n",
    "from config import lgbm_hyperparameter as lgbm_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(column):\n",
    "    text = ' '.join(column.astype(str))\n",
    "    words = text.upper().split()\n",
    "    word_counts = pd.Series(words).value_counts()\n",
    "    vocabulary = word_counts.index.tolist()\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_predictions(model, X_test, y_test, y_pred, probs, features, timestamp):\n",
    "    vectorizer_path = f\"../models/lgbm_{timestamp}/vectorizer.pkl\"\n",
    "    # Load the vectorizer from the file\n",
    "    with open(vectorizer_path, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    vocabulary_path = f\"../models/lgbm_{timestamp}/vocabulary.pkl\"\n",
    "    # Get the vocabulary of the training data\n",
    "    with open(vocabulary_path, 'rb') as f:\n",
    "        vocabulary = pickle.load(f)\n",
    "        \n",
    "    # Extrahieren der wichtigsten Features\n",
    "    boost = model.booster_\n",
    "    importance = boost.feature_importance()\n",
    "    feature_names = boost.feature_name()\n",
    "    sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "    feature_dict = {vocabulary.shape[0]+index: key for index, key in enumerate(features)}\n",
    "\n",
    "    true_label = y_test.reset_index(drop=True)\n",
    "\n",
    "    X_test_restored = vectorizer.inverse_transform(X_test[:,:vocabulary.shape[0]-len(features)])\n",
    "    original_designation = [' '.join(words) for words in X_test_restored]\n",
    "\n",
    "    print('Wichtigsten Features:')\n",
    "    for j in sorted_idx:\n",
    "        if importance[j] > 100:\n",
    "            if j < vocabulary.shape[0]:\n",
    "                print('{} ({}) Value: {}'.format(feature_names[j], importance[j], vocabulary[j]))\n",
    "            else:\n",
    "                print('{} ({}) Value: {}'.format(feature_names[j], importance[j], feature_dict[j]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Ausgabe der Vorhersagen, der Wahrscheinlichkeiten und der wichtigsten Features\n",
    "    for i in range(len(X_test)):\n",
    "        if y_pred[i] != true_label[i]:\n",
    "            if y_pred[i] == 1:\n",
    "                print('Vorhersage für Sample {}: Ja ({})'.format(i+1, y_pred[i]), 'True: Nein ({})'.format(true_label[i]))\n",
    "            else:\n",
    "                print('Vorhersage für Sample {}: Nein ({})'.format(i+1, y_pred[i]), 'True: Ja ({})'.format(true_label[i]))\n",
    "            print(original_designation[i])\n",
    "\n",
    "            print('Wahrscheinlichkeit für Sample {}: {}'.format(i+1, probs[i][1]))\n",
    "\n",
    "            print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_dataset(folder_path, only_text, augmentation, kfold):\n",
    "    dataframes_list = load_csv_into_df(folder_path, original_prisma_data=False, move_to_archive=False)\n",
    "    random.seed(33)\n",
    "    # Take random dataset from list as test set and drop it from the list\n",
    "    random_index = random.randint(0, len(dataframes_list) - 1)\n",
    "    ncar = ['G14', 'G15', 'G22', 'G23', 'G61', 'G65', 'NA5', 'NA7']\n",
    "    df_test = dataframes_list[random_index]\n",
    "    dataframes_list.pop(random_index)\n",
    "    logger.info(f\"Car {ncar[random_index]} is used to test the model on unseen data!\")\n",
    "    df_combined = combine_dataframes(dataframes_list)\n",
    "    df_preprocessed, df_for_plot = preprocess_dataset(df_combined, cut_percent_of_front=lgbm_params[\"cut_percent_of_front\"])\n",
    "    df_test, df_test_for_plot = preprocess_dataset(df_test, cut_percent_of_front=lgbm_params[\"cut_percent_of_front\"])\n",
    "\n",
    "    df_preprocessed.to_excel(\"df_preprocessed.xlsx\")\n",
    "    df_test.to_excel(\"df_test.xlsx\")\n",
    "\n",
    "    vocab = get_vocabulary(df_preprocessed['Benennung (bereinigt)'])\n",
    "\n",
    "    if augmentation:\n",
    "        # Declare which data augmentation techniques should be used\n",
    "        rand_order = True\n",
    "        rand_mistakes = False\n",
    "        gpt = True\n",
    "\n",
    "        # Generate the new dataset\n",
    "        df_preprocessed = data_augmentation(df_preprocessed, rand_order, rand_mistakes, gpt, df_to_excel = False)\n",
    "        df_preprocessed.to_excel(\"augmented_data.xlsx\")\n",
    "\n",
    "    weight_factor = round(df_preprocessed[df_preprocessed[\"Relevant fuer Messung\"]==\"Nein\"].shape[0] / df_preprocessed[df_preprocessed[\"Relevant fuer Messung\"]==\"Ja\"].shape[0])\n",
    "\n",
    "    dateTimeObj = datetime.now()\n",
    "    timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "    # Split dataset\n",
    "    if kfold:\n",
    "        X, y, X_test, y_test, features = train_test_val_kfold(df_preprocessed, df_test, only_text, test_size=lgbm_params[\"test_size\"], timestamp=timestamp)\n",
    "        return X, y, X_test, y_test, features, weight_factor, timestamp, vocab\n",
    "    else:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, features = train_test_val(df_preprocessed, df_test, only_text, test_size=lgbm_params[\"test_size\"], timestamp=timestamp)\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test, features, weight_factor, timestamp, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, weight_factor):\n",
    "    \n",
    "    class_weight = {0: 1, 1: weight_factor}\n",
    "    evals = {}\n",
    "    callbacks = [lgb.early_stopping(lgbm_params[\"early_stopping\"]), lgb.record_evaluation(evals)]\n",
    "\n",
    "    gbm = LGBMClassifier(boosting_type='dart',\n",
    "                        objective='binary',\n",
    "                        metric=['auc', 'binary_logloss'],\n",
    "                        num_leaves=lgbm_hp[\"num_leaves\"],\n",
    "                        max_depth=lgbm_hp[\"max_depth\"],\n",
    "                        learning_rate=lgbm_hp['lr'],\n",
    "                        feature_fraction=lgbm_hp[\"feature_fraction\"],\n",
    "                        n_estimators=lgbm_params[\"n_estimators\"],\n",
    "                        class_weight=class_weight)\n",
    "\n",
    "    gbm.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "            eval_metric=lgbm_params[\"metrics\"],\n",
    "            early_stopping_rounds=lgbm_params[\"early_stopping\"],\n",
    "            callbacks=callbacks)\n",
    "\n",
    "    \n",
    "    return gbm, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_trained_model(model, test_acc, timestamp):\n",
    "    # save model\n",
    "    model_path = f\"../models/lgbm_{timestamp}/model_{str(test_acc)[2:]}.pkl\"\n",
    "    with open(model_path, \"wb\") as filestore:\n",
    "        pickle.dump(model, filestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, evals, timestamp):\n",
    "    threshold = 0.75\n",
    "    probs = model.predict_proba(X_test)\n",
    "    y_pred = (probs[:,1] >= threshold)\n",
    "    y_pred =  np.where(y_pred, 1, 0) \n",
    "\n",
    "    # Print accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"\\n\\n Test accuracy:\", accuracy, \"\\n\\n\")\n",
    "\n",
    "    lgb.plot_metric(evals, metric='binary_logloss')\n",
    "    plt.savefig(f'../models/lgbm_{timestamp}/binary_logloss_plot.png')\n",
    "\n",
    "    lgb.plot_metric(evals, metric='auc')\n",
    "    plt.savefig(f'../models/lgbm_{timestamp}/auc_plot.png')\n",
    "\n",
    "    class_names = [\"Nein\", \"Ja\"]\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=class_names, cmap='Blues', colorbar=False)\n",
    "    plt.savefig(f'../models/lgbm_{timestamp}/confusion_matrix.png')\n",
    "\n",
    "    return y_pred, probs, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-05 16:56:05.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mipynb.fs.defs.Prepare_Data\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:13.798\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mipynb.fs.defs.Prepare_Data\u001b[0m:\u001b[36mload_csv_into_df\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1m8 dataframe(s) were created.\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:13.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_prepare_dataset\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mCar G22 is used to test the model on unseen data!\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:13.809\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mipynb.fs.defs.Prepare_Data\u001b[0m:\u001b[36mcombine_dataframes\u001b[0m:\u001b[36m89\u001b[0m - \u001b[32m\u001b[1m7 dataframe(s) are combined to one dataset.\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:13.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mipynb.fs.defs.Feature_Engineering\u001b[0m:\u001b[36mpreprocess_dataset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mStart preprocessing the dataframe with 20698 samples...\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:27.589\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mipynb.fs.defs.Feature_Engineering\u001b[0m:\u001b[36mpreprocess_dataset\u001b[0m:\u001b[36m363\u001b[0m - \u001b[32m\u001b[1mThe dataset is successfully preprocessed. The new dataset contains 4086 samples\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:27.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mipynb.fs.defs.Feature_Engineering\u001b[0m:\u001b[36mpreprocess_dataset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mStart preprocessing the dataframe with 3067 samples...\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:29.637\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mipynb.fs.defs.Feature_Engineering\u001b[0m:\u001b[36mpreprocess_dataset\u001b[0m:\u001b[36m363\u001b[0m - \u001b[32m\u001b[1mThe dataset is successfully preprocessed. The new dataset contains 557 samples\u001b[0m\n",
      "\u001b[32m2023-06-05 16:56:34.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mData_Augmentation\u001b[0m:\u001b[36mdata_augmentation\u001b[0m:\u001b[36m492\u001b[0m - \u001b[1mStart adding artificial designations...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main(crossvalidation: bool):\n",
    "    if crossvalidation == False:\n",
    "        # Split dataset\n",
    "        folder_path = \"../data/labeled_data/\"\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, features, weight_factor, timestamp, vocab = load_prepare_dataset(folder_path, only_text=False, augmentation=True, kfold=False)\n",
    "\n",
    "        store_model = False\n",
    "        show_preds = True\n",
    "\n",
    "        gbm, evals = train_model(X_train, y_train, X_val, y_val, weight_factor)\n",
    "        y_pred, probs, test_acc = evaluate_model(gbm, X_test, y_test, evals, timestamp)\n",
    "\n",
    "        if show_preds:\n",
    "            store_predictions(gbm, X_test, y_test, y_pred, probs, features, timestamp)\n",
    "\n",
    "        if store_model:\n",
    "            store_trained_model(gbm, test_acc, timestamp)\n",
    "\n",
    "        plot_importance(gbm, max_num_features=10)\n",
    "    else:\n",
    "        # Split dataset\n",
    "        folder_path = \"../data/labeled_data/\"\n",
    "        store_model = False\n",
    "        show_preds = False\n",
    "\n",
    "        X_train, y_train, X_test, y_test, features, weight_factor, timestamp, vocab = load_prepare_dataset(folder_path, augmentation=True, kfold=True)\n",
    "\n",
    "        kfold = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "        evals_list = []\n",
    "\n",
    "        for train_index, val_index in kfold.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            gbm, evals = train_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold, weight_factor)\n",
    "            evals_list.append(evals)\n",
    "\n",
    "            y_pred, test_acc = evaluate_model(gbm, X_test, y_test, evals, timestamp)\n",
    "\n",
    "        if show_preds:\n",
    "            store_predictions(gbm, X_test, y_test, y_pred, features, timestamp)\n",
    "\n",
    "        if store_model:\n",
    "            store_trained_model(gbm, test_acc, timestamp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main(crossvalidation=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
