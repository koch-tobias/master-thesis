{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_openai():\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_base = \"https://convaip-sbx-openai.openai.azure.com/\"\n",
    "    # openai.api_version = \"2022-12-01\" # For GPT3.0\n",
    "    openai.api_version = \"2023-03-15-preview\" # For GPT 3.5\n",
    "    openai.api_key = '9e6fa24631f54cf58866766bd31a2bff' #os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_order(description: str) -> str:\n",
    "    '''\n",
    "    This function takes a sentence as input and changes the word order randomly. \n",
    "    '''\n",
    "    # Split the description into individual words\n",
    "    words = description.split()\n",
    "    new_description = description\n",
    "\n",
    "    # Shuffle the words in the description\n",
    "    while new_description == description:\n",
    "        random.shuffle(words)\n",
    "\n",
    "        # Join the shuffled words back into a string\n",
    "        new_description = ' '.join(words)\n",
    "\n",
    "    return new_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_chars(description: str) -> str:\n",
    "    '''\n",
    "    This function takes a text as input and randomly swap two chars which are next to each other\n",
    "    '''\n",
    "    # Convert description to a list of chars\n",
    "    chars = list(description)\n",
    "\n",
    "    # Choose the chars which should be swaped \n",
    "    index1 = random.randrange(len(chars) - 1)\n",
    "    index2 = index1 + random.choice([-1,1])\n",
    "\n",
    "    # Change their position\n",
    "    chars[index1], chars[index2] = chars[index2], chars[index1]\n",
    "    \n",
    "    return ''.join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_char(description: str) -> str:\n",
    "    # German alphabet\n",
    "    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "    # Randomly choose a char of the alphabet\n",
    "    char = random.choice(alphabet)\n",
    "\n",
    "    # Randomly select the position where to add the char\n",
    "    index = random.randrange(len(description)+1)\n",
    "\n",
    "    # Add the selected char\n",
    "    new_description = description[:index] + char + description[index:]\n",
    "\n",
    "    return new_description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_char(description: str) -> str:\n",
    "    index = random.randrange(len(description))\n",
    "    new_description = description[:index] + description[index+1:]\n",
    "    \n",
    "    return new_description   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mistakes(description: str) -> str:\n",
    "    '''\n",
    "    This function takes a text as input and inserts a random spelling error. \n",
    "    This is done either by swapping chars, adding a chars, or deleting a chars.\n",
    "    '''\n",
    "    # Generate three random probabilities between 0 and 1 \n",
    "    probs = [random.uniform(0,1) for _ in range(3)]\n",
    "    probs = [p/sum(probs) for p in probs]\n",
    "\n",
    "    if probs[0] > (probs[1] and probs[2]):\n",
    "        # Randomly swap two chars\n",
    "        new_description = swap_chars(description)\n",
    "        \n",
    "        return new_description\n",
    "    \n",
    "    elif probs[1] > (probs[0] and probs[2]):\n",
    "        # Randomly add a char\n",
    "        new_description = add_char(description)\n",
    "\n",
    "        return new_description\n",
    "\n",
    "    else:\n",
    "        # Randomly delete one char\n",
    "        new_description = delete_char(description)\n",
    "        \n",
    "        return new_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text: str) -> str:\n",
    "    prompt = f'''\n",
    "                Task:\n",
    "                Write a modified car component designation based on the given German language input. Your output must be slightly different to the input while retaining the same meaning.\n",
    "\n",
    "                Context:\n",
    "                The original designation is written in German and may contain abbreviations and technical terms. You are not allowed to add or remove words. Abbreviations that often appear in the data set are 'HI' for 'HINTEN', 'VO' for 'VORN', 'HKL' for 'HECKKLAPPE','GPR' for GEPAECKRAUM', 'VKL' for 'VERKLEIDUNG', 'ISP' for 'INNENSPIEGEL', 'TV' for 'TUER VORNE', 'TH' for 'TUER HINTEN', 'HBL' for 'HOCHGESETZTE BREMSLEUCHTE', 'STF' for 'STOSSFAENGER', 'KST' for 'KOPFSTUETZE', 'ND' for 'NORMALDACH', 'DAHAUBE' for DACHANTENNENHAUBE. This abbreviation help you to create designations.\n",
    "\n",
    "                Instructions:\n",
    "                1. Carefully examine the input and understand its meaning.\n",
    "                2. Modify the designation to create a new version which is very close to the original.\n",
    "                3. Make sure that the meaning of the designation remains the same.\n",
    "                4. Make sure that the modified designation is not equal to the input data\n",
    "\n",
    "                Examples: \n",
    "                - Input Data: ABDECKLEISTE EINSTIEG HI\n",
    "                - Modified Designation: ABDECKLEISTE EINSTIEG HINTEN\n",
    "\n",
    "                - Original: KANTENSCHUTZ TUER VORN\n",
    "                - Modified Designation: KANTENSCHUTZ TV\n",
    "\n",
    "                - Original: MD KST VERSTELLBAR AUSSEN MAT\n",
    "                - Modified Designation: MD KST AUÃŸENMATTE VERSTELLBAR\n",
    "\n",
    "                - Original: SEITENVERKLEIDUNG\n",
    "                - Modified Designation: SEITENVERKL\n",
    "\n",
    "                - Orignal: ABDECKUNG FENSTERRAHMEN TUER VORNE\n",
    "                - Modified Designation: ABDECKUNG FENSTERRAHMEN TUERE VORN\n",
    "\n",
    "                \"\"\"\n",
    "                {text}\n",
    "                \"\"\"\n",
    "            '''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(response: str) -> str:\n",
    "    ''' \n",
    "    The output of GPT has always a prefix like \"Answer: \" or \"Modified Designation: \". This prefix will be deleted that we keep only the generated text.\n",
    "    '''\n",
    "    index = response.find(\":\")\n",
    "\n",
    "    if index != -1:\n",
    "        new_response = response[index+1:].lstrip()\n",
    "    else:\n",
    "        new_response = response\n",
    "    \n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt30_designation(text: str) -> str:\n",
    "    prompt= create_prompt(text)\n",
    "    new_response = text\n",
    "    while new_response == text:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"davinci-003-deployment\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            n=1\n",
    "        )\n",
    "        response = [choice.text.strip() for choice in response.choices]\n",
    "        new_response = remove_prefix(response[0])\n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt35_designation(text:str) -> str:\n",
    "    prompt= create_prompt(text)\n",
    "    new_response = text\n",
    "    while new_response == text:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"chat-gpt-0301\",\n",
    "            messages=[{\"role\":\"system\",\"content\":\"You are an AI assistant that helps to create a car component designation.\"},{\"role\":\"user\",\"content\": prompt}],\n",
    "            temperature=0.6,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            n=1\n",
    "        )\n",
    "        response = [choice.message.content for choice in response.choices]\n",
    "        new_response = remove_prefix(response[0])\n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_space(df):\n",
    "    x_min = df.loc[0, 'X-Min_transf']\n",
    "    x_max = df.loc[0, 'X-Max_transf']\n",
    "    y_min = df.loc[0, 'Y-Min_transf']\n",
    "    y_max = df.loc[0, 'Y-Max_transf']\n",
    "    z_min = df.loc[0, 'Z-Min_transf']\n",
    "    z_max = df.loc[0, 'Z-Max_transf']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['X-Min_transf'] < x_min:\n",
    "            x_min = row['X-Min_transf']\n",
    "        if row['X-Max_transf'] > x_max:\n",
    "            x_max = row['X-Max_transf']\n",
    "        if row['Y-Min_transf'] < y_min:\n",
    "            y_min = row['Y-Min_transf']\n",
    "        if row['Y-Max_transf'] > y_max:\n",
    "            y_max = row['Y-Max_transf']\n",
    "        if row['Z-Min_transf'] < z_min:\n",
    "            z_min = row['Z-Min_transf']\n",
    "        if row['Z-Max_transf'] > z_max:\n",
    "            z_max = row['Z-Max_transf']\n",
    "\n",
    "    # Add 10% to each side\n",
    "    expand_box_percent = 0.10\n",
    "    length = (x_max - x_min) * expand_box_percent\n",
    "    width = (y_max - y_min) * expand_box_percent\n",
    "    height = (z_max - z_min) * expand_box_percent\n",
    "    x_min = x_min-length\n",
    "    x_max = x_max+length\n",
    "    y_min = y_min-width\n",
    "    y_max = y_max+width\n",
    "    z_min = z_min-height\n",
    "    z_max = z_max+height\n",
    "\n",
    "    # Define the corner matrix\n",
    "    corners = np.array([[x_min, y_min, z_min],\n",
    "                [x_min, y_min, z_max],\n",
    "                [x_min, y_max, z_min],\n",
    "                [x_min, y_max, z_max],\n",
    "                [x_max, y_min, z_min],\n",
    "                [x_max, y_min, z_max],\n",
    "                [x_max, y_max, z_min],\n",
    "                [x_max, y_max, z_max]])\n",
    "\n",
    "    return corners, length, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_centerpoint_in_valid_space(corners):\n",
    "    x_min, y_min, z_min = corners[0]\n",
    "    x_max, y_max, z_max = corners[7]\n",
    "    x = np.random.uniform(x_min, x_max)\n",
    "    y = np.random.uniform(y_min, y_max)\n",
    "    z = np.random.uniform(z_min, z_max)\n",
    "    return np.array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box_limits(center_point, length, width, height, theta_x, theta_y, theta_z):\n",
    "    # Convert Euler angles to rotation matrix\n",
    "    r = Rotation.from_euler('xyz', [theta_x, theta_y, theta_z], degrees=True)\n",
    "    R = r.as_matrix()\n",
    "\n",
    "    # Define vertices of bounding box in local coordinate system\n",
    "    vertices = np.array([\n",
    "        [-length/2, -width/2, -height/2],\n",
    "        [length/2, -width/2, -height/2],\n",
    "        [-length/2, width/2, -height/2],\n",
    "        [length/2, width/2, -height/2],\n",
    "        [-length/2, -width/2, height/2],\n",
    "        [length/2, -width/2, height/2],\n",
    "        [-length/2, width/2, height/2],\n",
    "        [length/2, width/2, height/2]\n",
    "    ])\n",
    "\n",
    "    # Apply rotation matrix to vertices to transform to global coordinate system\n",
    "    transformed_vertices = np.dot(vertices, R.T)\n",
    "\n",
    "    # Calculate minimum and maximum values of x, y, and z coordinates\n",
    "    x_min = np.min(transformed_vertices[:, 0]) + center_point[0]\n",
    "    x_max = np.max(transformed_vertices[:, 0]) + center_point[0]\n",
    "    y_min = np.min(transformed_vertices[:, 1]) + center_point[1]\n",
    "    y_max = np.max(transformed_vertices[:, 1]) + center_point[1]\n",
    "    z_min = np.min(transformed_vertices[:, 2]) + center_point[2]\n",
    "    z_max = np.max(transformed_vertices[:, 2]) + center_point[2]\n",
    "\n",
    "    return x_min, x_max, y_min, y_max, z_min, z_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minmax(list_transformed_bboxes):\n",
    "    x_min = np.inf\n",
    "    x_max = -np.inf\n",
    "    y_min = np.inf\n",
    "    y_max = -np.inf\n",
    "    z_min = np.inf\n",
    "    z_max = -np.inf\n",
    "\n",
    "    for arr in list_transformed_bboxes:\n",
    "        x_coords = arr[:, 0]\n",
    "        y_coords = arr[:, 1]\n",
    "        z_coords = arr[:, 2]\n",
    "        \n",
    "        x_min = min(x_min, np.min(x_coords))\n",
    "        x_max = max(x_max, np.max(x_coords))\n",
    "        y_min = min(y_min, np.min(y_coords))\n",
    "        y_max = max(y_max, np.max(y_coords))\n",
    "        z_min = min(z_min, np.min(z_coords))\n",
    "        z_max = max(z_max, np.max(z_coords))\n",
    "\n",
    "    return x_min, x_max, y_min, y_max, z_min, z_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_boundingbox(df_original, df_temp):\n",
    "    corners, valid_length, valid_width, valid_height = find_valid_space(df_original)\n",
    "    list_corners = []\n",
    "    list_corners.append(corners)\n",
    "    valid_x_min, valid_x_max, valid_y_min, valid_y_max, valid_z_min, valid_z_max = get_minmax(list_corners)\n",
    "\n",
    "    # Generate random values for length, width, and height until the volume is within the desired range\n",
    "    min_volume = df_original['volume'].min()\n",
    "    max_volume = df_original['volume'].max()\n",
    "    volume = 0\n",
    "    x_min = 0\n",
    "    x_max = 0\n",
    "    y_min = 0\n",
    "    y_max = 0\n",
    "    z_min = 0\n",
    "    z_max = 0\n",
    "    length = 0\n",
    "    width = 0\n",
    "    height = 0\n",
    "    logger.info(f\"{min_volume}, {max_volume}, {valid_x_max}, {valid_x_min}, {valid_y_max}, {valid_y_min}, {valid_z_min}, {valid_z_max}, {valid_length}, {valid_width}, {valid_height}\")\n",
    "    \n",
    "    #while ((volume < min_volume) or (volume > max_volume)):\n",
    "    center_point = random_centerpoint_in_valid_space(corners)\n",
    "    length = np.random.uniform(1, valid_length)\n",
    "    width = np.random.uniform(1, valid_width)\n",
    "    height = np.random.uniform(1, valid_height)\n",
    "    x_min, x_max, y_min, y_max, z_min, z_max = get_bounding_box_limits(center_point, length, width, height, df_temp[\"theta_x\"], df_temp[\"theta_y\"], df_temp[\"theta_z\"])\n",
    "    volume = length * width * height\n",
    "    logger.info(f\"{length}, {width}, {height}, {x_min}, {x_max}, {y_min}, {y_max}, {z_min}, {z_max}, {volume}\")\n",
    "\n",
    "    # Modify the bounding box information\n",
    "    df_temp.loc[0, \"X-Min_transf\"] = x_min\n",
    "    df_temp.loc[0, \"X-Max_transf\"] = x_max\n",
    "    df_temp.loc[0, \"Y-Min_transf\"] = y_min\n",
    "    df_temp.loc[0, \"Y-Max_transf\"] = y_max\n",
    "    df_temp.loc[0, \"Z-Min_transf\"] = z_min\n",
    "    df_temp.loc[0, \"Z-Max_transf\"] = z_max\n",
    "    df_temp.loc[0, \"center_x\"] = center_point[0]\n",
    "    df_temp.loc[0, \"center_y\"] = center_point[1]\n",
    "    df_temp.loc[0, \"center_z\"] = center_point[2]\n",
    "    df_temp.loc[0, \"length\"] = length\n",
    "    df_temp.loc[0, \"width\"] = width\n",
    "    df_temp.loc[0, \"height\"] = height\n",
    "    df_temp.loc[0, \"volume\"] = volume\n",
    "    df_temp.loc[0, \"density\"] = volume/df_temp.loc[0, \"Wert\"]\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_old(df: pd.DataFrame, rand_order:bool, rand_mistakes:bool, gpt:bool, df_to_excel:bool) -> pd.DataFrame:\n",
    "    ''' \n",
    "    This function generates synthetic data to extend the data set. \n",
    "    Only the data points that are relevant for a measurement are expanded, since this class is very underrepresented. \n",
    "    A component name is expanded if it contains more than 2 words and at least one of the three data augmentation techniques is activated.\n",
    "    Return: New dataset \n",
    "    '''\n",
    "    init_openai()\n",
    "\n",
    "    logger.info(\"Start adding artificial designations...\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"Sachnummer\"], ignore_index=True)\n",
    "\n",
    "    # Create a new empty dataframe with the same columns as the original dataframe\n",
    "    new_df = df.iloc[[0]]\n",
    "\n",
    "    # Iterate over the rows in the original dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        # Copy the orignal sample to a temporary dataframe\n",
    "        temp_df = df.iloc[[i]]\n",
    "        \n",
    "        # Add a new sample to the new dataframe\n",
    "        if i>0:\n",
    "            new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "\n",
    "        if row[\"Relevant fuer Messung\"]==\"Ja\":\n",
    "            if len(row[\"Benennung (dt)\"].split()) > 2:\n",
    "                if rand_order:\n",
    "                    # Generate a new description by random ordering and adding the new sample to the dataset\n",
    "                    temp_df.loc[i,\"Benennung (dt)\"] = random_order(row[\"Benennung (dt)\"])\n",
    "                    new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "\n",
    "                if rand_mistakes:\n",
    "                    # Create a new description by incorporating random mistakes and adding the new sample to the dataset\n",
    "                    temp_df.loc[i,\"Benennung (dt)\"] = random_mistakes(row[\"Benennung (dt)\"])\n",
    "                    new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "            if len(row[\"Benennung (dt)\"].split()) > 1:\n",
    "                if gpt:\n",
    "                    # Create a new description with GPT and adding the new sample to the dataset\n",
    "                    temp_df.loc[i,\"Benennung (dt)\"] = gpt35_designation(row[\"Benennung (dt)\"])\n",
    "                    if len(temp_df[\"Benennung (dt)\"][i]) < 40:\n",
    "                        new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "            else:\n",
    "                continue\n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    if df_to_excel == True:\n",
    "        dateTimeObj = datetime.now()\n",
    "        timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "        new_df.to_excel(f\"../data/artificial_dataset_{timestamp}.xlsx\")\n",
    "\n",
    "    logger.success(f\"Creating a new dataset with artificial designations was succeccfull!\")\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_augmentation\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame, rand_order:\u001b[39mbool\u001b[39m, rand_mistakes:\u001b[39mbool\u001b[39m, gpt:\u001b[39mbool\u001b[39m, df_to_excel:\u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    This function generates synthetic data to extend the data set. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m    Only the data points that are relevant for a measurement are expanded, since this class is very underrepresented. \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    A component name is expanded if it contains more than 2 words and at least one of the three data augmentation techniques is activated.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m    Return: New dataset \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     init_openai()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def data_augmentation(df: pd.DataFrame, rand_order:bool, rand_mistakes:bool, gpt:bool, df_to_excel:bool) -> pd.DataFrame:\n",
    "    ''' \n",
    "    This function generates synthetic data to extend the data set. \n",
    "    Only the data points that are relevant for a measurement are expanded, since this class is very underrepresented. \n",
    "    A component name is expanded if it contains more than 2 words and at least one of the three data augmentation techniques is activated.\n",
    "    Return: New dataset \n",
    "    '''\n",
    "    init_openai()\n",
    "\n",
    "    logger.info(\"Start adding artificial designations...\")\n",
    "\n",
    "    df_relevant_parts = df[df[\"Relevant fuer Messung\"] == \"Ja\"]    \n",
    "\n",
    "    unique_names = df_relevant_parts[\"Einheitsname\"].unique().tolist()\n",
    "    unique_names.sort()\n",
    "    for name in unique_names:\n",
    "        df_new = df_relevant_parts[(df_relevant_parts[\"Einheitsname\"] == name)].reset_index(drop=True)\n",
    "        df_temp = df_new.iloc[[0]]\n",
    "\n",
    "        if df_new.shape[0] < 3:\n",
    "            if len(df_temp.loc[0,\"Benennung (bereinigt)\"].split()) > 1:\n",
    "                df_temp.loc[0,\"Benennung (bereinigt)\"] = random_order(df_new.loc[0,\"Benennung (bereinigt)\"])\n",
    "                if df_temp.loc[0,\"volume\"] > 0:\n",
    "                    df_temp = augmented_boundingbox(df_new, df_temp)  \n",
    "                    df = pd.concat([df, df_temp]).reset_index(drop=True)   \n",
    "\n",
    "                df_temp.loc[0,\"Benennung (bereinigt)\"] = gpt35_designation(df_new.loc[0,\"Benennung (bereinigt)\"])\n",
    "                if len(df_temp[\"Benennung (bereinigt)\"][0]) < 40 and df_temp.loc[0,\"volume\"] > 0:\n",
    "                    df_temp = augmented_boundingbox(df_new, df_temp)\n",
    "                    df = pd.concat([df, df_temp], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    dateTimeObj = datetime.now()\n",
    "    timestamp = dateTimeObj.strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "    df.to_excel(f\"../data/artificial_dataset_{timestamp}.xlsx\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the path to the labeled dataset\n",
    "    data = pd.read_excel(\"../data/G65_bounding_pp.xlsx\", index_col=0)\n",
    "    \n",
    "    # Declare which data augmentation techniques should be used\n",
    "    rand_order = True\n",
    "    rand_mistakes = True\n",
    "    gpt = True\n",
    "\n",
    "    # Generate the new dataset\n",
    "    new_data = data_augmentation(data, rand_order, rand_mistakes, gpt, save_as_excel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-02 16:43:16.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_augmentation\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mStart adding artificial designations...\u001b[0m\n",
      "\u001b[32m2023-05-02 16:43:39.885\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_augmentation\u001b[0m:\u001b[36m52\u001b[0m - \u001b[32m\u001b[1mCreating a new dataset with artificial designations was succeccfull!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
