@misc{yenduriGenerativePretrainedTransformer2023,
  title = {Generative {{Pre-trained Transformer}}: {{A Comprehensive Review}} on {{Enabling Technologies}}, {{Potential Applications}}, {{Emerging Challenges}}, and {{Future Directions}}},
  shorttitle = {Generative {{Pre-trained Transformer}}},
  author = {Yenduri, Gokul and M, Ramalingam and G, Chemmalar Selvi and Y, Supriya and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and G, Deepti Raj and Jhaveri, Rutvij H. and B, Prabadevi and Wang, Weizheng and Vasilakos, Athanasios V. and Gadekallu, Thippa Reddy},
  year = {2023},
  month = may,
  number = {arXiv:2305.10435},
  eprint = {2305.10435},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.10435},
  urldate = {2023-10-16},
  abstract = {The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\q617269\\Zotero\\storage\\RTBVHBAC\\Yenduri et al. - 2023 - Generative Pre-trained Transformer A Comprehensiv.pdf;C\:\\Users\\q617269\\Zotero\\storage\\PPREVBL7\\2305.html}
}
