@misc{deroosProbabilisticallyMotivatedLearning2021,
  title = {A {{Probabilistically Motivated Learning Rate Adaptation}} for {{Stochastic Optimization}}},
  author = {{de Roos}, Filip and Jidling, Carl and Wills, Adrian and Sch{\"o}n, Thomas and Hennig, Philipp},
  year = {2021},
  month = feb,
  number = {arXiv:2102.10880},
  eprint = {2102.10880},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-05},
  abstract = {Machine learning practitioners invest significant manual and computational resources in finding suitable learning rates for optimization algorithms. We provide a probabilistic motivation, in terms of Gaussian inference, for popular stochastic first-order methods. As an important special case, it recovers the Polyak step with a general metric. The inference allows us to relate the learning rate to a dimensionless quantity that can be automatically adapted during training by a control algorithm. The resulting meta-algorithm is shown to adapt learning rates in a robust manner across a large range of initial values when applied to deep learning benchmark problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\q617269\\Zotero\\storage\\HEMUIP5Y\\de Roos et al. - 2021 - A Probabilistically Motivated Learning Rate Adapta.pdf;C\:\\Users\\q617269\\Zotero\\storage\\U4L9MJBC\\de Roos et al. - 2021 - A Probabilistically Motivated Learning Rate Adapta.pdf;C\:\\Users\\q617269\\Zotero\\storage\\S2MUKM6J\\2102.html}
}
